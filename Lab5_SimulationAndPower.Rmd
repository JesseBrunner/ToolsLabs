---
title: 'Lab: Making Up Sh*t up:Simulation and Power analysis'
author: "Jesse Brunner"
date: '`r format(Sys.Date())`'
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---


```{r, echo=FALSE}
knitr::opts_chunk$set(fig.width = 4, fig.height = 3)
```


Simulating data: Signal (deterministic model) + Noise (stochastic distribution)
--------------------------------------------------------

In previous labs we've explored deterministic functions and probability distributions. Now let’s put the two together to derive (= make up) new data under a specific model. Let’s assume our ecological process of interest can be represented by a linear model with normally distributed errors (e.g., productivity as a function of nitrogen concentration over a small range). Our mathematical formula is:
$$
  \begin{aligned}
  Y \sim \text{Normal}(a+bx, \sigma^2)
  \end{aligned}
$$

which means that $Y$ is a random variable drawn from a normal distribution with mean $a + bx$ and variance $\sigma^2$. To simulate data under this model we will take two steps. First, we need to calculate the deterministic process/dynamics/outcome. To do this we must set up the values of $x$ and specify  values for the parameter $a$ and $b$:
 
```{r x}
x <- 1:20

a <- 2 
b <- 1
```

We can then calculate the deterministic part of the model:

```{r ydet}
( y_det <- a + b * x)
```

Next, to get our simulated "data" we need to add some stochastic noise to the deterministic expectations (i.e., the expected value or mean for each value of `y_det`) with the `rnorm()` function. The basic syntax is `rnorm(n, mean, sd)` and again, the mean is our expected value or `y_det`. 
To generate 20 random normal deviates with the mean equal to the deterministic equation and $sigma$ = 2 we can write:
```{r set.seed, echo=FALSE}
set.seed(1001)
```

```{r rnorm.1}
y <- rnorm(20, mean = y_det, sd = 2)
```
Note that we could achieve the same results with:
```{r rnorm.2}
y <- rnorm(20, mean = a + b*x, sd = 2)
y
```
Just to be clear, let's see how the stochastic version of our simple model differs from the deterministic one. Plot your simulated observations as points against $x$ and then add a best-fit linear regression line to those points. Add a line for the "True" relationship (your deterministic function) to see how a function estimated from a given set of observations might deviate from the true function. 

```{r rnorm.2.plot}
library(tidyverse) 
ggplot(data.frame(x, y, y_det), aes(x=x, y=y)) + 
	geom_point() + 
	geom_smooth(method = "lm") + # a linear regression fit to the data
	geom_line(aes(y=y_det), color = "red") # the True relationship
```
Not bad! 

(*Note*: Your values will differ from these because of the randomness of our draws from this probability distribution. We can set the initial random number the same using `set.seed(1)` or whatever number you like. This can be useful for trouble-shooting a stochastic model.)

We can more formally fit a regression to our simulated data to see how whether we can recover the original, "True", parameters, or at least get close. 

```{r lm.1}
l1 <- lm(y ~ x)
l1
```
So we can see that our estimates of the parameters $a$ (=`intercept`) and $b$ (=`x`, since it's the effect of $x$ on $y$) are pretty close. But there's not much returned. We can get much more information by using the `summary()` command on the `lm` object. 
```{r lm.2}
summary(l1)
```
That's much more useful. We can see that the estimates of these parameters are within a standard error of the "True" values. We can construct confidence intervals more formally by remembering that the confidence intervals are  $\approx \bar{x} \pm 1.96\times\sigma/\sqrt{n}$. Where does the $1.96$ come from? Remember, we want 5% in the tails of the distribution, 2.5% in each side. We can use the quantile function to tell us what values along the x-axis are associated with the 2.5% and 97.5% quantiles.

```{r qnorm}
# The 97.5th percentile 
qnorm(1 - 0.05/2)
# and the 2.5th percentile of the normal
qnorm(0.05/2)
```

We can then use this information to calculate the confidence intervals on the parameters $a$ and $b$.
```{r confint.1}
# a
2.251 + 1.96*1.234
2.251 - 1.96*1.234
# b
0.989 + 1.96*0.103 
0.989 - 1.96*0.103 
```

Or alternately, we can use the built-in `confint()` function, which does the work for us.
```{r confint.2}
confint(l1)
```
You will notice, however, that this provides different, wider confidence intervals than what we just calculated ourselves. Why? Well, for `lm()` models, `confint()` correctly uses the critical values from the _t_-distribution. As you may well remember, the _t_-distribution converges on the standard normal (with mean = 0 and sd = 1) when the degrees of freedom is large. In our case, with $df = 20 - 2$, the proper confidence intervals will be a bit wider. (Try it yourself using `qt()` for the _t_-distribution and see if you can match the output from the `confint()` function.) Anyway, back to the more important point...

If we were to run this process again, how many times would you expect simply by random chance that we’d estimate parameters different from our true model? What if you increased the standard deviation, say to $\sigma$ = 6? Give it a shot! Here is one example:

```{r lm.s6.plot}
ggplot(data.frame(x=x, y=rnorm(20, mean = a + b * x, sd = 6)), aes(x,y)) + 
	geom_point() + 
	geom_smooth(se = FALSE, linetype = 2) + # a loess curve
	geom_smooth(method = "lm") + # a linear regression
	stat_function(fun = function(x) a + b*x) # the True relationship
```

Try repeating this a bunch of times. Do  you always get something close to the "right" answer? What if we didn't have our x-values so evenly distributed? If you didn't know what the right answer was, would you always assume that a linear model was the best choice? Consider the different biological interpretations that could mean. Troubling isn’t it?!



Why simulate data?
------------------

So now we have made up some data (= Signal + Noise), but it is worth stepping back and thinking about why this is helpful. There are lots of reasons, in fact. Making up data help you:

1.  Think through the process(es) generating data in your system
2.  See what your data might look like
3.  "Sharpen your intuition and test your estimation tools."
4.  Calculate the power (and bias and precision of estimates) for an experimental design, given effect sizes you care about
5.  Bracket uncertainty in a process of interest (e.g., bootstrap CIs)
6.  Explore the consequences of certain decisions (e.g., over fitting)

In this lab we are going to focus on #4 and #6. Numbers 1-3 are most useful when you do this for your own system. Number 5 is a special type of resampling statistic that we won't worry about at present; just file it away for later. But power analyses, well those are really, really useful. You should start to think about simulating data for your design _before_ you conduct your study. They can give you a much better sense of whether your study design is adequate for what you want to find out. Wouldn't you rather find out that your experiment cannot do what you need it to do _before_ you do all the hard work? Or even better, identify a different approach that _will_ work?

Power analysis: continuing our linear example
--------------------------------------------

You now have (almost) all of the basic tools you'll need to calculate power, bias, and other useful metrics. All that is left is to learn the how to do what we just did by hand, hundreds of times. 

We are going to stick with our simple linear regression model, not so much because we expect really interesting results, but because it is a case most people have some sense for and so we can focus the mechanics for a bit while on comfortable ground. We know intuitively that the power of our experiment will increases with sample size, with the effect size (the true difference in means), and the critical value we use (although in practice, $alpha$ is fixed at $0.05$), and decreases as the variation (i.e., noise around our data) increases. 

So let's imagine that we are planning an experiment, like we simulated above, with 20 equally-spaced values of our predictor ($x$) and we want to see:

1. How likely we are to correctly say that the slope is significantly different from zero (i.e., correctly reject the null hypotheses; this is the classic definition of power), and
2. Whether we correctly recover the True value of the slope ($b = 1$).

What this means, in practice, is that we simulate our experimental data, fit a linear model to it, and keep both the P-value (for aim 1) and estimate of the slope (for aim 2), then repeat this a bunch of times.  

*We will build up the function in layers.* So what you will see first is a skeleton of the function, and then we will fill in the necessary code in several steps.  First, let's specify what parameters the function will take and what it will return. In our case, we will give it a set of x-values, an intercept, a slope, a standard deviation, and the number of simulations to run. I've given everything a default value that matches our example above. We need only supply the vector of x-values. At the end, the function will return a data frame with a column of P-values (`pvals`) and a column with the estimates of the slope (`slopes`). Of course we need to store those values somewhere inside the function, so I create two numeric vectors of the appropriate length. 

```{r est_lm_power.fxn.1, eval=FALSE}
est_lm_power <- function(x, intercept = 2, slope = 1, sd = 5, nsim = 500){

	# create two vectors to store the p-values and estimates 
	# of the slope from each simulated experiment
	pvals <- slopes <- numeric(length = nsim)

return( data.frame(pvals, slopes) )	
	
}
```

Next, we need our function to create the deterministic expectation and then add the appropriate amount of noise to it to create our fake data. We need only create the deterministic expectation once, but we want to create many (n = `nsim`) sets of fake data, so I put the fake data creation stuff inside a loop. 
```{r est_lm_power.fxn.2, eval=FALSE}
est_lm_power <- function(x, intercept = 2, slope = 1, sd = 5, nsim = 500){
	
	# create two vectors to store the p-values and estimates 
	# of the slope from each simulated experiment
	pvals <- slopes <- numeric(length = nsim)
	
	# calculate the deterministic expectation
	# Note that x must be provided by the user. 
	# It remains the same in each iteration
	y_det <- intercept + slope * x
	
	# each of the nsim loops is a simulated experiment
	for(i in 1:nsim){
		
		# create simulated data with noise determined by sd
		y <- rnorm(n=length(x), mean = y_det, sd = sd)
	}

return( data.frame(pvals, slopes) )	
	
}
```
The `for(i in 1:nsim){}` is a loop that starts at `i=1`, runs whatever is in the curly brackets, then increments `i` to `i=2`, runs the stuff in the curly brackets again, and on and on through `i=nsim`. It's useful to remember that the value of `i` actually changes. We will use this to index our results in the next chunk of code. (It is also worth noting that there are other control-flow structures, including `while(condition){}`, `if(condition){}`, and `switch(expression, alt1, alt2, ...)`. The R language is much more efficient if you avoid these constructs, but sometimes they are just plain helpful!)

The last step in our function is to fit a regression model to our made up data and extract the _P_-value and estimate of the slope from it. The first part just uses the `lm()` function, as we've seen. The second part uses the `coef()` function, which helpfully extracts the coefficients of the regression model, along with their standard errors, t-test values, and P-values. We simply select the row we want (that for "x") and the correct columns. If you don't understand what I'm extracting, try fitting a regression to one set of made up data and then using the `coef(summary())` function on it. 
```{r est_lm_power.fxn}
est_lm_power <- function(x, intercept = 2, slope = 1, sd = 5, nsim = 500){
	
	# create two vectors to store the p-values and estimates of the slope from each simulated experiment
	pvals <- slopes <- numeric(length = nsim)
	
	# calculate the deterministic expectation
	# Note that x must be provided by the user. 
	# It remains the same in each iteration
	y_det <- intercept + slope * x
	
	# each of the nsim loops is a simulated experiment
	for(i in 1:nsim){
		
		# create simulated data with noise determined by sd
		y <- rnorm(n=length(x), mean = y_det, sd = sd)

		# fit the linear regression model
		model <- lm(y~x)

		# extract the p-value for the regression slope ("x")
		pvals[i] <- coef(summary(model))["x","Pr(>|t|)"]
		# and the estimated slope for "x"
		slopes[i] <- coef(summary(model))["x","Estimate"]
	}

return( data.frame(pvals, slopes) )	
	
}
```

So what proportion of times do we correctly reject the null hypothesis? That is, what is the power of our experiment?

```{r power.1, cache=TRUE}
p1 <- est_lm_power(x)
with(p1, sum(pvals < 0.05) / length(pvals))
```
Wow! That's pretty good! We are almost certain to correctly reject the null hypothesis, provided the underlying model and variance are accurate! What about our estimate of the slope? Do we do a good job of estimating the slope?

```{r slope.1}
summary(p1$slopes)
ggplot(p1, aes(x=slopes)) + 
	geom_histogram(binwidth = 0.02) + 
	geom_vline(xintercept=1)
```

That's not bad, either! Fifty percent of the time we are within 0.15 of the "True" value, so even with a sample size of just 20 we have pretty good precision.. Moreover, our mean and median are very nearly 1, which is to say that our estimate doesn't seem to be biased.  But what if we were interested in smaller slopes? How small a slope could we detect with reasonable confidence? Back to our simulations!

Since we already have a function that simulates an experiment a whole bunch of times given a set of parameters (`est_lm_power()`), why don't we simply feed this function a bunch of values for the slope?

```{r est_lm_power_slope}
est_lm_power_slope <- function(x, intercept=2, slope=seq(0,1, length=10), sd=5, nsim=500){
	
	# set up the results data frame
	results <- data.frame(pvals = numeric(), 
			slopes = numeric(), 
			true_slope = numeric())
	
	# loop through all of the slope values
	for(i in 1:length(slope)){
		# get the results from the nsim "experiments"
		current_results <- est_lm_power(x, intercept=intercept, 
				slope=slope[i], 
				sd=sd, 
				nsim=nsim)
		#add a column to the data frame with the true slope
		current_results$true_slope <- slope[i]
		
		# add the results from the current loop to the master results data frame
		results <- rbind(results, current_results)	
	}

	return(as.data.frame(results))
}
```

You will notice that this function takes the same inputs, only `slope` is now a vector of slope values. We can thus change any of the particular values within the simulation if we wish. Since the `est_lm_power()` function returns a data frame, our function simply takes that data frame and adds it to the bottom of the growing `results` data frame.

```{r power.2, cache=TRUE}
p2 <- est_lm_power_slope(x, slope = seq(0,1, length = 11))

# Find the power calculated by each level of true_slope
p2 %>% 
	group_by(true_slope) %>% 
	summarise(Power = sum(pvals < 0.05)/n())
# Plot the power by true_slope... Here we use stat_summary() to do the math
# but we could have kept the previous summary and plotted that
ggplot(data = p2, aes(x = true_slope, y = pvals)) + 
	stat_summary(fun.y = function(y) sum(y < 0.05)/length(y), geom = "point")
```

So, as we'd expect, if the true difference in slope from zero is small, we have fairly low power to detect that difference. Of course how small a difference is important is up to you to determine!  (Also, it is worth noting that even when there is _no_ difference, we observe a power > 0. Think about why that is.) But there we go, our first power analysis!


Power analysis II: using the power of the Tidyverse!
--------------------------------------------
The above version of of our power analysis works just fine, but we can perhaps do better in terms of performance and easy of maintaining/modifying the code. On the first point, note that loops in R are really inefficient. If we could modify our code to work with vectors or data.frames rather than loops we can gain a bit of performance. Also, is we use a slightly different approach it becomes a lot easier to pull out the things we want.

```{r powerTidy}
# create a data.frame with all of the combinations of 
# variables over which we want to simulate data. This will be big!
df <- expand.grid(x=1:20,
		intercept=2, 
		slope=seq(0,1, length=10), 
		sd=5, 
		sim=1:500)
dim(df)
# Now simulate data based on these variables
df$y <- with(df, rnorm(n=length(x), mean=intercept+slope*x, sd=sd))
```

So we have created the simulated data sets across all 500 simulations for each of the 10 levels of slope. Doing this all at once instead of in a loop is usually much, much faster (often an order of magnitude). But still, how do we now fit a regression to each simulation at each level of slope? The answer is that we can first group our long data frame using the `group_by()` function, and then apply an arbitrary function using the `do()` function. But there is one more secret, we can use the `tidy()` function in the `broom` package to get the regression outputs in a nice, tidy data frame! 
```{r powerTidy2, cache=TRUE}
# install.packages("broom") # run if it's not installed
library(broom)

by_rep <- df %>% 
	group_by(slope, sim) %>% # group our data by slope and simulation number
	do(tidy( lm(y~x, data=.) )) # apply a data frame to each grouping

# Take a look at this new data frame
by_rep
```
Notice that we now have columns for the estimate and p.value, along with a lot of other things, for the two terms (intercept and slope) of each model. (If you want to get out other bits of information like $r^2$, AIC, etc., check out the `glance` function and see the vignette for `broom`.)
Now the last step is to calculate power.
```{r powerTidy3, cache=TRUE}
by_rep %>% filter(term=="x") %>% # just want the slopes, not intercepts
	group_by(slope) %>% 
	summarise(power = sum(p.value < 0.05)/n()) # or equivalently: mean(p.value < 0.05))
```

Putting it all together:
```{r powerTidyFxn}
est_lm_power_slope2 <- function(x, intercept=2, slope=seq(0,1, length=10), sd=5, nsim=500){ 
	require(tidyr)
	require(broom)
	
	# create data frame of deterministic components
	df <- expand.grid(x=x, intercept=intercept, slope=slope, sd=sd,	sim=1:nsim)
	
	# add in column of simulated data
	# NOTE: could also use mutate to do the same
	df$y <- with(df, rnorm(n=length(x), mean=intercept+slope*x, sd=sd))
	
	# group, fit model, extract p.values, calculate power
	by_rep <- df %>% 
		group_by(slope, sim) %>% # group our data by slope and simulation number
		do(tidy( lm(y~x, data=.) )) %>% # fit model to each group
		filter(term=="x") %>% # extract terms of slopes, not intercepts
		group_by(slope) %>% # group by slopes
		summarise(power = sum(p.value < 0.05)/n()) # calculate power
	
	return(by_rep)
}
```

Which version is faster?
```{r compareTimes, cache=TRUE}
system.time(est_lm_power_slope(x))
system.time(est_lm_power_slope2(x))
```
Ack! Our tidy version is a bit _slower_! But it is easier to work with and maintain, or adapt to new models. And I think there will be places where it is faster (not sure where this code is getting slowed down... <sigh>).


********

You probably won't be surprised to learn that R has several built in functions for calculating traditional power for common experimental designs/statistics:

function             | notes
-------------------- | -------------
`power.t.test()`     | tests difference between means of two normal populations
`power.prop.test()`  | tests difference in proportions
`power.anova.test()` | tests for a balanced, one-way ANOVA

But for more complicated and ecologically realistic examples (i.e., most of the time) you’ll need to find the answer through simulation.

*******

Fitting, over fitting, and prediction
------------------------------------

There is a problem we will run up against soon: over fitting. The idea is that we can add in extra parameters and our model will fit better---it has to!---but these better fitting models will be _worse_ at predicting the what would happen in a new data set.  
Let us begin by simulating data from a moderately complex model:
$$ y \sim Normal(mean= b_0 + b_1 x_1 + b_2 x_2 + b_3  x_3, sd) $$
We will simulate some values of $x_1, x_2,$ and $x_3$, which are all in the True model, but also $x_4$, which is not. We will then simulate the $y$ values given our linear model. So this creates our data set of measured variables (our $x$'s) and the response variable, $y$.
```{r}
set.seed(2)
b0 <- 10
b1 <- 1.5
b2 <- -1
b3 <- 0.75
sd <- 5
# our initial data set, to which we will fit our models
df1 <- data.frame(x1=runif(n=20, min=10, max=100), 
				x2=runif(n=20, min=5, max=20),
				x3=runif(n=20, min=-8, max=8),
				x4=runif(n=20, min=0, max=50)) # notice: not in True model
df1$y <- with(df1, rnorm(n=20, mean=b0 + b1*x1 + b2*x2 + b3*x3, sd=sd))
df1
```

Now let us fit four different models to our data set. Since we do not know the True model, we try using models with all four variables in succession. (Note, we could actually fit $4! = `r factorial(3)`$ models if we used all combinations of variables! But let's keep this simple.) 
```{r}
# fit linear models with 1, 2, 3, or all 4 terms (plus intercept)
summary(lm1 <- lm(y ~ x1, data=df1))
summary(lm2 <- lm(y ~ x1 + x2, data=df1))
summary(lm3 <- lm(y ~ x1 + x2 + x3, data=df1))
summary(lm4 <- lm(y ~ x1 + x2 + x3 + x4, data=df1))
```

Notice which terms are significant and which models have the greatest $R^2$. Which model would you choose if you didn't know the Truth?

OK, but here' the point. Imagine we collected new observations (here 2000, so that we have less random outcomes and thus a clearer point to make). Notice that the range of measured predictor variables is a bit wider, but not much. 
```{r}
# data set we will predict
df2 <- data.frame(x1=runif(n=2000, min=0, max=120), 
				x2=runif(n=2000, min=0, max=35),
				x3=runif(n=2000, min=-10, max=10),
				x4=runif(n=2000, min=0, max=50))
df2$y <- with(df2, rnorm(n=200, mean=b0 + b1*x1 + b2*x2 + b3*x3, sd=sd))
```

Lastly, let's predict what our response variable, $y$ _should_ be based on each of the four models. Then we can measure the variance between the actual observe $y$ values, given our true deterministic model + the same amount of noise, and the values predicted by the models.
```{r}
df2$preds1 <- predict(lm1, newdata=df2)
df2$preds2 <- predict(lm2, newdata=df2)
df2$preds3 <- predict(lm3, newdata=df2)
df2$preds4 <- predict(lm4, newdata=df2)

df2 <- df2 %>% 
	gather(key="model", value="predicted", preds1, preds2, preds3, preds4)

df2 %>% group_by(model) %>% 
	summarize(variance= sum( ((y-predicted)^2)/(n()-1) ) )
```
See that the variance is greater for model 4, which includes a term that is _not_ in the underlying True process/model. While that term gave our model 4 enough flexibility to better fit the data in the first data set, even if just a bit, it led to poor predictions for new data. Step back and think about why this is...

Now look at model 3. Is it a better fit than the simpler models? Why or why not? 

Try repeating this whole process again (just run it, don't change things) and see how often the True model is the best model in terms of prediction.

This is, in a nut shell, what we're up against with fitting and over fitting. We want to find a parsimonious explanation for our data, but also be able to predict new data (i.e., make our results generalizable to new situations). For now we'll leave it there, but if your curious, try playing with the terms in the underlying model (e.g., change the parameter values or make an $x^2$ term) and see when we are at greatest risk of over fitting. 


Homework
----------
So we have seen the basics of simulation and power analyses, broadly defined, in two semi-realistic, but kind of boring examples. Your homework is to try to simulate data for an experiment or study of your own. You might choose a study you've already done or one you're thinking of doing. Be explicit about what you want to learn from the simulation and what you need to simulate across. If you get stuck, talk to your friends and collaborators first, then ask me. While I'd like you to go all of the way through the exercise, even if you just start setting up the problem it can be helpful.


