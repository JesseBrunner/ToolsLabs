Baysian Analyses with WinBUGS (and some of that): EMD chapter 6
========================================================

_Modifed from a lab by Elizabeth Hunter (2011), with help from Marc Kery (2010) and Ben Bolker (2008)_

In this lab we will be applying a Bayesian approach to model fitting using the same myxomatosis dataset and model that was the homework in the previous likelihood lab.  Historically we would follow Bolker's example and use the program **WinBUGS** (**B**ayesian inference **U**sing the **G**ibbs **S**ampler), but there is a lot of development in the Bayesian world. WinBUGS has now been superceded by [OpenBUGS](http://www.openbugs.info/) in most respects and is still be actively developed, whereas WinBUGS is not. More importantly, at least for our purposes, these are both written for PCs. Although it is possible to run them on Linux or Mac OSX using [Wine](http://www.winehq.org/), that can introduce a lot of stupid headaches that I wish to avoid. Instead we are going to use an open-source, cross-platform (and perhaps faster) program called [JAGS](http://mcmc-jags.sourceforge.net/) (**J**ust **A**nother **G**ibbs **S**ampler). The underlying approach and code among is largely identical between these three---indeed, all are variants of the BUGS language---so the WinBUGS code in Bolker's book should work without modification. The `rjags` package works a bit differently than `R2WinBUGS`, though, so keep an eye out for that.  

Installing JAGS and rjags
------------------

1. Download the JAGS program at http://mcmc-jags.sourceforge.net/   
Make sure you get the 3.3.0 version 
2. Follow the installation instructions... nothing special here

3. Install the `rjags` package that lets R talk to JAGS. In RStudio use "Install Packages" in the  "Tools" menu and type in `rjags` as you would with any other package. 


Writing a BUGS model
-------------------
Let us first set out our goal. We wish to fit a Ricker model to a subset of the myxomatosis data using a Bayesian approach. We will assume that the data are Gamma distributed around this deterministic expectation. This mirrors the likelihood approach you used for your homework in the last lab. As a refresher, though, remember that the Ricker model is:

\begin{aligned}
  f(x) & = ax \times exp(-bx)
\end{aligned}

where $a$ is the initial slope and the maximum value is found at $1/b$, and the mean of the Gamma distribution is $shape / rate$.

How do we code this in the BUGS language? 

```
model {
	for (i in 1:n) {
	  # define the expected value at each day
		mean [ i ] <- a*day[ i ]*exp(-b*day[ i ])
		# convert this mean into the rate paramter
		rate [ i ] <- shape/mean[ i ]
		
		# observed titers are gamma distributed with these deterministic rates and shape parameters 
		titer [ i ] ~ dgamma (shape, rate[ i ])
	}
##priors
a ~ dgamma (0.1, 0.1)
b ~ dgamma (0.1, 0.1)
shape ~ dgamma (0.01, 0.01)
}
```


**Model:** The syntax in WinBUGS is very similar to [R], but there are a few key differences.  Most important, assignment must always be done by the arrow (`<-`) instead of an equals sign for deterministic functions and the tilde (`~`) is used for stochastic relationships.  Second, unlike [R] BUGS does cannot perform an operation on a whole vector at once; we will always have to explicitly loop through the vector. Here we loop through each of the `n` observations each time calculating the mean or expected value of the titer from the Ricker model, then using this mean (and a shape parameter that we specify elsewhere) to calculate the rate parameter of the Gamma distribution, and lastly specifying that our actual data ('titer') are gamma distributed with a shape (which is constant) and rate parameter we've just defined.   

**Priors:** In the model code, you must also specify your priors.  Here we are using relatively vague priors, meaning that the probability is spread out over many x-values.  The Gamma distribution is always parameterized as shape and rate in BUGS/JAGS. (Note that many stochastic distributions in BUGS/JAGS have somewhat odd parameterizations, like using "precision" = 1/variance). To see just how these priors are shaped we can plot them in R.

```{r plot.gammas, fig.width=5, fig.height=4}
library(ggplot2)
qplot(x = c(0,50), stat = "function", fun = dgamma, args = list(shape = 0.1, rate = 0.1))
qplot(x = c(0,50), stat = "function", fun = dgamma, args = list(shape = 0.01, rate = 0.01))
```
Notice that there is still a peak at smaller values, which is reasonable given what we want a Ricker to do, but all sorts of values are also possible. If we were worried about this particular set of priors, we could (and should!) re-run the model with different types of priors.


Running JAGS through R
-------------------------

While it is possible to run JAGS from the command line, it is usually invoked from [R] using the `rjags` package. We have JAGS do all of the Gibbs sampling wizardry and then analyze our results in [R]. It's the best of both worlds. So let's do it.

First, we let us load the myxomatosis data. The data do not _need_ to have the same names as in the model file, but it helps keep everything straight.

```{r load.myxo}
library(emdbook)
data(MyxoTiter_sum)
head(MyxoTiter_sum)
myxdat <- subset(MyxoTiter_sum, grade==1)

titer = myxdat$titer
day = myxdat$day
n = length(titer)
```

We then generate lists of starting values for the parameters/
```{r inits.myxo}
inits <- list(list(a=4, b=0.2, shape=90), 
							list(a=1, b=0.1, shape=50), 
							list(a=8, b=0.1, shape=150))
```
Note that we need three different sets of starting values here because we are going to run three different mcmc (Markcov Chain Monte Carlo) chains. We use multiple, independent chains so that we can be sure that our results do not depend on the starting conditions. This is just like choosing different starting values in a likelihood-optimizization routine; we'd hate to be stuck on a local maximum and not know it.  While JAGS can come up with random initial values, they may not be very reasonable, and this can produce errors. In general you will be better off if you specify initial values (use the methods of moments to come up with reasonable values).  

Next, we need write out the model structure in a separate text file. We can do this directly through R and save it to a text file using the `sink()` function. This model is exactly the same as the one we used before.
```{r sink.model.myxo, eval=FALSE}
sink("myxomodel.txt")
cat("
model {
	for (i in 1:n) {
		mean [ i ] <- a*day[ i ]*exp(-b*day[ i ])
		rate [ i ] <- shape/mean[ i ]
		titer [ i ] ~ dgamma (shape, rate[ i ])
	}
##priors
a ~ dgamma (0.1, 0.1)
b ~ dgamma (0.1, 0.1)
shape ~ dgamma (0.01, 0.01)
}
", fill=TRUE)
sink()

```


Now we are ready to create a jags model (from the text file we just created), with the appropriate data and initial conditions, and  run it.
```{r myxo1.jags, eval=FALSE}
library(rjags)
dataList <- list("titer" = titer, "day" = day, "n" = n)

myxo1.jags <- jags.model("myxomodel.txt",
												 data = dataList,
												 chains = 3,
												 inits = inits)
```


```{r myxo1.jags.inline, echo=FALSE, warning=FALSE}
### Have to do this b/c RStudio creates a clean environment 
# when it runs the RMarkdown stuff and the model file 
# doesn't exist in this new environment

modelstring <- "
model {
	for (i in 1:n) {
		mean [ i ] <- a*day[ i ]*exp(-b*day[ i ])
		rate [ i ] <- shape/mean[ i ]
		titer [ i ] ~ dgamma (shape, rate[ i ])
	}
##priors
a ~ dgamma (0.1, 0.1)
b ~ dgamma (0.1, 0.1)
shape ~ dgamma (0.01, 0.01)
}
"
library(rjags)
dataList <- list("titer" = titer, "day" = day, "n" = n)

myxo1.jags <- jags.model(textConnection(modelstring),
                   data = dataList,
                   n.chains = 3,
                   inits = inits)
```

Next we want to give the mcmc chains a little while to converge on the posterior distribution from the initial parameters we provided. Think of it this way: if the posterior distribution is a mountain, our initial starting values are random locations on or near the mountain. We need to give our chains time to travel up the mountain to the highest (most likely) points.
```{r myxo1.burnin}
update(myxo1.jags, n.iter = 1000) # Let it "burn in" 1000 steps
```

Now that the mcmc chains have burned in a bit (i.e., they are now sampling the posterior distribution), let's collect a large set of samples from the posterior with which to estimate our parameters and associated credible intervals. We are using the 'coda.samples()` function to update our `myxo.jags` model, keeping `n.iter` samples from the random walk of the mcmc chains around the posterior distribution of each parameter we care about. 
```{r myxo1.coda, cache=TRUE}
myxo1.coda <- coda.samples(myxo1.jags,
             variable.names = c("a", "b", "shape"),
             n.iter = 10000) #This is the number of steps we want to save 
```

Let's see what is inside this new sample from the mcmc chains.
```{r myxo1.coda.head}
head(myxo1.coda)
```

Here we see the first seven entries of each of the three chains we started. Each entry (row) is one random value from the posterior distribution of the parameters (columns). Of course what we really want are means or medians and associated confidence intervals. 
```{r myxo1.coda.summary}
summary(myxo1.coda)
```
So the best estimate of $a$ is `r round(summary(myxo1.coda)$statistics[1,1], 3)`. There is a 95% _probability_ that $a$ is between `r round(summary(myxo1.coda)$quantiles[1,1], 3)` and `r round(summary(myxo1.coda)$quantiles[1,5], 3)`. You can find the same information for the other two parameters. (Note that this averages across the three chains.)

We can see the posterior distributions of each parameter (as well as the traces of the chains, which are the values of each chain for a given parameter at each point in time) by simply plotting them.
```{r myxo1.coda.plot}
plot(myxo1.coda)
```

Notice that our chains largely cover one another. This is a good sign as it indicates that they have converged. If they did not overlap, that would suggest that they are sampling different regions of the posterior, each suggesting different solutions for our model. At a minimum if our mcmc chains have not converged the Bayesian credible intervals we calculate will be too wide. Worse, it might mean that our model is poorly structured, not parameterized well, or that our data (and priors) are not informative. If your model does not converge you can run it longer (it will converge on the right answer as the chain goes to infinity, if that makes you feel better) or try some different initial values. Or you may need to go back and think more carefully about your model structure. 

In any case, we may want a more precise diagnostic for convergence. The Gelman and Rubin "potential scale reduction factor" is just such a test. For our (elementary) purposes, convergence is achieved if the upper confidence interval is very near 1. If it is substantially higher, then our model has not converged. 
```{r myxo1.coda.gelman}
gelman.diag( myxo1.coda )
```
In this case we're just fine.

It is also useful to understand how our parameters correlate with one-another.
```{r myxo1.coda.crosscorr}
crosscorr(myxo1.coda)
```

It is clear that in our model parameters $a$ and $b$ are highly corrleated with one another. Still, wouldn't you like to see it for yourself?

First we want to combined all of our chains together. The `as.matrix()` function applied to `coda` objects does thust this. We then convert it into a data frame.
```{r myxo1.df}
myxo1.df <- as.data.frame( as.matrix(myxo1.coda) )
```

We can then plot the posterior estimates of each parameter against the others.
```{r myxo1.df.plots, fig.width=5, fig.height=4, warning=FALSE}
library(ggplot2)
qplot(a, b, data = myxo1.df, alpha = I(1/20)) + geom_smooth() + theme_bw()
qplot(a, shape, data = myxo1.df, alpha = I(1/20)) + geom_smooth() + theme_bw()
qplot(b, shape, data = myxo1.df, alpha = I(1/20)) + geom_smooth() + theme_bw()
```



