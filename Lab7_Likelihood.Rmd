---
title: 'Lab7: Fitting models to data part 2: likelihood'
author: "Jesse Brunner"
date: '`r format(Sys.Date())`'
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.width = 4, fig.height = 3)
```


This week we will learn to fitting models to data using likelihood as our criterion for fit. Specifically our goals are to:

* calculated the likelihood of a simple data set by hand
* learn to use R functions to calculate likelihood for you
* find the parameter values that maximize the likelihood
* automating the maximizing of likelihood with optim()


A simple example, from class
----------------------------

In class I gave you two problems to work through by hand, with a calculator. Let's revisit these examples using R for the heavy lifting. 

The first problem was this: imagine we collected 9 wood frog tadpoles from a pond, brought them back to lab, and found that 4 were infected. What is our best estimate of the probability of infection in the pond, which we will call $\pi$? In likelihood we simply flip this on its head, asking what value of $\pi$ maximizes the probability of seeing our data? The idea is that the parameter value that is most likely to give us our data is the most likeliy, given our data. 

For our data, 4 out of 9 infected (4 successes in 9 Bernouli trials) the likelihood is provided by the binomial: 

$$ \mathcal{L}(\pi | \mbox{4 of 9}) = P(\mbox{4 of 9}| \pi) = \binom {9}{4} \pi^4 (1-\pi)^{9-4}. $$

Notice that $\pi$ is the only parameter in the equation; the data are fixed. So we can choose a value of $\pi$ and then calculate its likelihood, and repeat until we find a value that gives us the maximum $\mathcal{L}$. For example, the likelihood of $\pi = 0.1$ is

$$ \mathcal{L}(\pi = 0.1 | \mbox{4 of 9}) = P(\mbox{4 of 9} | \pi = 0.1) = \binom {9}{4} 0.1^4 (1-0.1)^5. $$

In R we can calculate this as (remembering that $\binom {9}{4} = \frac{9!}{4!(9-4)!}$)
```{r likelihood_1}
factorial(9)/(factorial(4) * factorial(5)) * 0.1^4 * 0.9^5
# which is the same as
choose(n=9, k=4) * 0.1^4 * 0.9^5
```

However you will recall that R has built-in functions to provide the probability (density or mass), like `dbinom()`. For this data set, the probability is
```{r likelihood_2}
dbinom(x=4, size=9, prob=0.1)
```
This makes it a lot easier to try lots of values as well as plot the likelihood as a function of different values of $\pi$. 
```{r likelihood_plot}
pis <- 1:99/100 # vector of values of Pi
Ls <- dbinom(x=4, size=9, prob=pis) # vector of likelihoods
library(ggplot2)
ggplot(data=data.frame(pis, Ls), aes(x=pis, y=Ls)) + geom_point() + geom_vline(xintercept=4/9)
```

So we can see that the Likeihood is maximized at $\pi = 4/9 = 0.444444$. 

**Model 1: A frog's a frog**
To complicate things a bit, I added a second sample of green frog tadpoles in which 2 of 5 were infected. We are assuminf that the probability of infection, $\pi$, is the same between species (i.e., $\pi_{WF} = \pi_{GF} = \pi$). The likelihood of both samples (the first AND the second) is just the product of the binomial probability of each sample:
$$ 
  \begin{aligned}
  \mathcal{L} (\pi | \mbox{4 of 9} \cap \mbox{2 of 5}) 
  &= P(\mbox{4 of 9} | \pi ) \times P( \mbox{2 of 5} | \pi) 
  &= \binom {9}{4} \pi^4 (1- \pi)^5 \times \binom {5}{2} \pi^2 (1 - \pi)^3
  \end{aligned}
$$

In general the likelihood of multiple samples, each of size $n_i$ with probability of success, $\pi$, is just the product of the likelihood of each sample: 
$$
	 \mathcal{L} = \prod\limits_{i=1}^N \binom{n_i}{k_i} \pi^{k_i}(1-p)^{n_i-k_i}.
$$

So to make sure you can do this, calculate the likelihood of this full data set with the two samples with R. (Note, the answer should be `r round(dbinom(x=4, size=9, prob=6/14)*dbinom(x=2, size=5, prob=6/14), 4)`. It may help to remember that if a frog is a frog then our $\pi_{MLE}$ is just the total number infected out of the total number tested.)`

As we observed in class (and you can quickly verify for yourself using R---try multiplying all of the numbers of infected and tested by 10), the likelihood values get smaller as the sample size increases. This causes computational problems when these products of small numbers get closer and closer to zero. So instead we usually work in terms of the log-likelihood (LL), which is:
$$
	 LL = \sum\limits_{i=1}^N \left( \log  \binom{n_i}{k_i} +k_i\log \pi + (n_i-k_i) \log(1-\pi) \right)
$$
Note that working with the log-likelihood we now work with the sums. This makes it much easier to calculate L for larger sample sizes. In R we can simply calculate the LL by specifying `log=TRUE` in the `dbinom()` function. 

So our log-likelihood for the above example is:
```{r logLik}
dbinom(x=4, size=9, prob=6/14, log=TRUE) + dbinom(x=2, size=5, prob=6/14, log=TRUE)
``` 
And the plot we created above is 
```{r loglikelihood_plot}
LLs <- dbinom(x=4, size=9, prob=pis, log=TRUE) + dbinom(x=2, size=5, prob=pis, log=TRUE)
ggplot(data.frame(pis, LLs), aes(pis, LLs)) + geom_point() + geom_vline(xintercept=6/14)
```

In actual fact, though, we tend to work on the _negative_ log-likelihood (NLL) because minimization problems are a bit easier to code than maximization problems (just like we did with sums-of-squares) and because this quantity is used in metrics of fit like AIC. It is honestly just a matter of putting a minus sign in front of the equation or code.

```{r NLL_plot}
-( dbinom(x=4, size=9, prob=6/14, log=TRUE) + dbinom(x=2, size=5, prob=6/14, log=TRUE) )
NLLs <- -( dbinom(x=4, size=9, prob=pis, log=TRUE) + dbinom(x=2, size=5, prob=pis, log=TRUE) )
ggplot(data.frame(pis, NLLs), aes(pis, NLLs)) + geom_point() + geom_vline(xintercept=6/14)
```

Again, we put a vertical line at $\pi_{MLE}$. It may be difficult for you to see that this is really the minimum (i.e., the MLE), but computurs are pretty good at figuring out these small differences. 

Finding the maximum likelihood or minimum NLL with `optim()`
-----------------------------------------------

Ideally (and soon, necessarily) we want to automate the way we find MLEs. In outline, it is very similar to what we did with the sums-of-squares before. In fact, we will start with the 'optim()' function again before learning a slightly snazzier function. 

First we need a function that takes our data (both $n$ & $k$ for each sample) and our parameter(s), here $\pi$ and then returns the NLL.
```{r NLL_fxn}
binom.NLL <- function(n, k, pi){
	-sum( dbinom(x=k, size=n, prob=pi, log=TRUE) )
}
```

Let's make sure that this works with our data from the example above.
```{r NLL_fxn_test}
tested <- c(9,5)
infected <- c(4,2)
binom.NLL(n=tested, k=infected, pi=6/14)
```
OK, the function works. Now we can try to use 'optim()` to automatically find $\pi_{MLE}$, which should be 6/14 = `r 6/14`. 
```{r NLL_optim}
optim(fn=binom.NLL, par=c(pi=0.5), n=tested, k=infected, 
			method="Brent", lower=0.1, upper=1) 
```
Success! So we can automagically find the MLE for our simple model! Please note two things before moving on. 1) We are poviding intitial conditions with the named vector, `par=c(pi=0.5)`. This gives the fitting algorithm a reasonable place from which to start. In simple cases like this we could start most anywehere and the algorithm would be OK, but in most cases with more complex likelihood surfaces the algorithm is likely to veer off in a bad direction if we start it from somewhere stupid. 2) We are specifying a method (here, Brent, which is best for one-dimensional optimization...try it without this line and see what happens) and some bounds on the paremeter (this method requires that you provide boundaries that give finite values for the likelihood). You can actually have a lot of control over the workings of `optim()` 

```{r NLL_optim1}
optim(fn=binom.NLL, par=c(pi=0.5), n=tested, k=infected, 
			method="Nelder-Mead", # yes, this will give an error
			control = list(trace=2) ) # this gives you details on the iterations inside optim()
```

In the longer run, `optim()` has some rough edges and makes a few things we'd like to do a bit difficult. Thankfully, Bolker has written a package called `bbmle` that has function, `mle2()`, that can fit likelihood models for us (it calls `optim()` for us and takes care of a lot of niggly bits). In outline it works the same, but it prefers both a list (rather than a vector) of initial starting values and a list of our data. 
```{r NLL_mle2}
library(bbmle) # you will probably have to install this package first
mle2(binom.NLL, start=list(pi=0.5), data=list(n=tested, k=infected))
```
Why do we get slightly different values with `mle2()` than from `optim()`? It comes down to different default optimization algorithms. We could, however, get the same exact answer if we said `method="Brent, lower=0.1, upper=1` in the call to `mle2()`. It is, afterall, just a nice wrapper around the call to `optim()`. So they may seem very similar, but in the longer run `mle2()` has some very nice bells and whistles, so we will stick with it.

**Model 2: A wood frog is not a green frog**
So the alternative model is that the probability of infection is different for each species, that is $\pi_{wf} \neq \pi_{gf}$. Overall, our likelihood looks like it did above, only with two different $\pi$s.
$$ 
  \begin{aligned}
  \mathcal{L} (\pi_{wf}, \pi_{gf} | \mbox{4 of 9} \cap \mbox{2 of 5}) 
  &= P(\mbox{4 of 9} | \pi_{wf} ) \times P(\mbox{2 of 5} | \pi_{gf}) 
  &= \binom {9}{4} \pi_{wf}^4 (1- \pi_{wf})^5 \times \binom {5}{2} \pi_{gf}^2 (1 - \pi_{gf})^3
  \end{aligned}
$$

How do we code this in R? Well, we just need to change our `binom.NLL()` function to accomodate the two sets of data with different parameters. We need it to take as an argument a vector telling it which group is which (here, `spp`) and then also the two parameters. Within the function, then, it just needs to use the right parameter for each species/group. 

```{r NLL_fxn2}
binom.NLL2 <- function(n, k, spp, piWF, piGF){
	-sum( dbinom(x=k, size=n, 
				prob=ifelse(spp == "WF", piWF, piGF), 
				log=TRUE) )
}
```
Notice that now prob is either `piWF` (when `spp == "WF"` is `TRUE`) or `piGF` (when `spp == "WF"` is `FALSE`). Let's see what we get.

```{r NLL_fxn2_mle2}
# create a vector species
spp <- c("WF", "GF")
# use mle2 to fit the model with _two_ parameters to the data
mle2(binom.NLL2, 
		 start=list(piWF=0.5, piGF=0.5), 
		 data=list(n=tested, k=infected, spp=spp))
```
We should see that the estimate of $\pi_{wf}$ is the same as the MLE for $\pi$ in the first dataset with only wood frogs. The estimate for $\pi_{gf}$ should confirm your intuition, too.

**An easier way**: The way I just showed you is robust and will always work. But there is a second, often easier way of doing the same thing. It is especially handy for doing things like calculating confidence intervals on parameters. So it is worth learning.

First, however, we need to learn a bit about we can specify our deterministic and stochastic models using the _formula_ interface in `mle2()` Let's see the example for model 1, with a single common value of $\pi$. 
```{r mle2_wizardry_1}
mle2(infected ~ dbinom(size=tested, prob=pi),
		 start=list(pi=0.5), 
		 data=list(tested, infected)
		 )
```
The formula is `infected ~ dbinom(size=tested, prob=pi)`, which reads, "the number infected is binomially distributed with the `size` of each trial equal to the number tested and a probability (`prob`) of "success" that is a constant called `pi`." (If you look at the help file for `dbinom()` you will see that the first argument it takes is `x`, the number of "successes". The `mle2()` function knows that whatever is on the left side of the equation (left of the tilde) is this `x`. Everything else needs to be inside the `dbinom()` part of the equation. Also note that `mle2()` knows to set `log=TRUE` when it calls `dbinom()`. ) So this formula notation simplifies things a bit and, provided you are careful, can make your code a lot cleaner. It won't work in every case, but it is a good bit more flexible than I've shown (and as we'll see later). 

Now on to how to have different parameters for each group. _If_ we use the formula notation, it becomes fairly simple. We can make the parameter `pi` a linear function of some other variable, like `spp`. 
```{r mle2_wizardry_2, eval=FALSE}
mle2(infected ~ dbinom(size=tested, prob=pi),
		 start=list(pi=0.5), 
		 data=list(tested, infected, spp), #add "spp" to the data
		 parameters=list(pi ~ spp) # add a "parameters" option saying that "pi" is a function of "spp"
		 )
```
Note the two things I've added. First, I added `spp` to the data list. Second, and most importantly, I added the `parameters` option. This uses the same formula notation as linear regressions, etc. Here I said that the parameter `pi` varies with `spp`, but we could just as easily make it a function of multiple variables, e.g., `parameters=list(pi ~ spp + age + pond)` if we provided data on the age of the individual and the pond, etc. 

The results of this call look like this:
```{r hidden, echo=FALSE, warning=FALSE}
a <- mle2(infected ~ dbinom(size=tested, prob=pi),
		 start=list(pi=0.5), 
		 data=list(tested, infected, spp), #add "spp" to the data
		 parameters=list(pi ~ spp) # add a "parameters" option saying that "pi" is a function of "spp"
		 )
a
```

You may be a bit confused by the output. You get a `pi.(Intercept)` and a parameter called `pi.sppWF`. This follows the linear regression notation where there is an intercept for the "base" group (here, the first level of `spp`, which is defined, by default alphabetically, as `GF`) and then an added effect of being in the other group (`spp` = `WF`). You can certainly recover the same values we observed before (with a bit of rounding error). $\pi_{gf}$ = `pi.(Intercept)` = `r coef(a)[1]` and $\pi_{wf}$ = `pi.(Intercept)` + `pi.sppWF` = `r coef(a)[1]` + `r coef(a)[2]`= `r sum(coef(a))`. However, if we wanted we could instead fit this model without an intercept, which would yeild estimates of $\pi$ for each species separately. To do so we use the `-1` to say "no intercept".
```{r mle2_wizardry_3, warning=FALSE}
mle2(infected ~ dbinom(size=tested, prob=pi),
		 start=list(pi=0.5), 
		 data=list(tested, infected, spp), 
		 parameters=list(pi ~ -1 + spp) 
		 )
```

Note that we can also specify more complex relationships within the formula interface. For instance, imagine we expected the probability of infection would increase logistically with the size of the tadpole, here snout-vent-length or SVL:  
$$ P(infection) = \frac{1}{1+e^{-(b_0 + b_1 \times \text{SVL})}}.$$
We could then write a call to `mle2()` something like:
```{r complex_formula, eval=FALSE}
mle2(infected ~ dbinom(size=tested, prob=1/(1+exp(-(b0 + b1*SVL)) ) ),
		 start=list(b0=0.5, b1=0), 
		 data=list(tested, infected, SVL)
		 )
```
Note that we would have to specify the initial conditions for the two parameters, `b0` and `b1`. So you can start to see how easily you can link your data to a model with different deterministic and stochastic parts. 


An example with two parts to the likelihood!
-------------------------------------

One of the advantages of the flexible likelihood approach we're using is that we can combined different sorts of data into a single likelihood calculation. (Of course this sort of approach does _not_ work well with the formula notation! Sorry!) Imagine that you were interested in pollination of a certain rare flower. Specifically you wanted to estimate pollinator visitation rates ($\lambda$) as well as the probability that a flower is fertilized given a visit ($\phi$) so that you can get a sense of whether these flowers are pollinator-limited. _Note that we are assuming in this example that these flowers cannot self!_

Estimating pollinator visitation rates is straightforward, if not easy. Just sit in a lawn chair with a beer in hand recording how many visits each of several focal flowers get per day (or hour or whatever). Take a guess: how would these visitation data be distributed? 

The second part is a bit more difficult to estimate in nature (although I'm sure you could imagine running around with a Q-tip and pollen in the lab or other similar experiments). Generally what you could do is measure the number or fraction of flowers that produced seeds, assuming that any polinated flowers would produce seeds. How do you think these data would be distributed? 

The key to the problem is to realize that the probability of being fertilized (and thus having a seed, ignoring the possible events after fertilization) is a function of both the total number of visits to a flower (which is a function of visitation rate times the time, $t$, the flower is open) and the probability of being pollinated given a visit. We might start by writing down:

$$
	P(pollinated) = \lambda \times t \times \phi,
$$

where $t$ is the time the flowers are open to pollinators, but in fact this would not quite be right. Once a flower is pollinated, it can't be pollinated again, so none of the extra visits that might have been successful count. So instead what we want to do is calculate the probability that a flower is _not_ pollinated and subtract that from one, which is

$$
	P(pollinated) = 1-(1-\phi)^{\lambda \times t}.
$$

Make sure you follow the logic before you move on.

Right, so back to our data. We have 10 observations of visitation rates (numbers of visits to a flower per day):
```{r data1}
visits <- c(1, 0, 1, 0, 2, 1, 2, 1, 0, 0)
```
and we also came back and collected 15 samples of 20 flowers each to measure the proportion that were fertilized:
```{r data2}
fert <- c(16, 13, 15, 15, 10, 13, 13, 20, 17, 19, 10, 13, 15, 12, 14)
```

So let's be clear about the likelihood of each part. Assuming that flowers are visited randomly by pollinators, we would expect our data to be Poisson distributed with a rate paramter $\lambda$ (I sort of telegraphed that, didn't I?) and the fertilization data would be binomially distributed with a probability of success = $P(pollinated)= 1-(1-\phi)^{\lambda \times t}$. For simplicity let's assume that all flowers are open for exactly 3 days. 

Let's build up our likelihood analysis in a few steps, starting with the visitation data. We need a function that gives us the NLL of our data (`visits`) given a value of $\lambda$. 
```{r pollination_visits_NLL}
visits.NLL <- function(visits, lambda){
	-sum( dpois(x=visits, lambda=lambda, log=TRUE) )
}
```
This should look fairly similar to what we did in the previous examples. If you are getting lost, compare this to `binom.NLL()` above. 

So let's find the $\lambda_{MLE}$.
```{r pollination_visits_MLE}
mle2(visits.NLL, start=list(lambda=1), data=list(visits=visits))
```
```{r echo=FALSE,warning=FALSE}
lam <- mle2(visits.NLL, start=list(lambda=1), data=list(visits=visits))
```

We can use the same approach to estimate the probability a flower is pollinated from the `fert` data.
```{r pollination_fert_NLL, warning=FALSE}
# our NLL function assuming binomially distributed data
fert.NLL <- function(fert, size, probFert){
	-sum( dbinom(x=fert, size=size, prob=probFert, log=TRUE) )
}
# find the MLE of probFert
mle2(fert.NLL, start=list(probFert=0.5), data=list(fert=fert, size=20))
```
```{r echo=FALSE,warning=FALSE}
p <- mle2(fert.NLL, start=list(probFert=0.5), data=list(fert=fert, size=20))
```


So our best estimate of the visitation rate is `r round(coef(lam),4)` per day and our best estimate of the probability of fertilization over the 3d window is `r round(coef(p),4)`. Now how do we use these estimates to figure out the pollination probability, $\phi$? Well the naive approach, which is commonly used and reasonably acceptable, would be to plug these numbers into the equation for the probability of fertilization, which we set out above, and solve for $phi$:
$$
P(pollinated) = 1-(1-\phi)^{\lambda \times t} = 
`r round(coef(p),4)` = 1-(1-\phi)^{`r round(coef(lam),4)` \times 3}.
$$
By this method, $\phi =$ `r round( 1-(1-coef(p))^(1/(coef(lam)*3)), 4)`, which isn't a bad guess. It ignores the fact, though, that there is information on $\lambda$ in the `fert` data. That is, the `fert` data can, and should, be used to constrain our best guess at what $\lambda$ should be and thus what $\phi$ is. So how do we combined these data sets to estimate both parameters simultaneously? It is actually not too difficult. We just need to calculate the NLL for each part and then add them together. (Remember, we are interested in the probability of seeing the `visits` data AND the `fert` data, so we need to multiply their likelihoods, but because we are working on the _log_-likelihood scale, we just add them.)
```{r pollination_combined_NLL}
poll.NLL <- function(visits, lambda, fert, size, phi){
	# neg log-likelihood of Poisson-distributed visits
	NLL.visits <- -sum( dpois(x=visits, lambda=lambda, log=TRUE) )
	
	# calculate probFert
	probFert <- 1-(1-phi)^(lambda*3)
	# neg log-likelihood of binomially-distributed fertalization data
	# given a probability of fertilization as calculated
	NLL.fert <- -sum( dbinom(x=fert, size=size, prob=probFert, log=TRUE) )
	
	return(NLL.visits + NLL.fert)
}
```
(Notice that we calculate the `probFert` value using both `phi` and `lambda`, but those values are whatever is given to the function by `mle2()`, they do not change. We are _not_ first finding the MLE of `lambda` and then using that to calculate `probFert`; we could calculate `probFert` right at the start and it would work the same.)

We'll get some errors because the optimization routine is going below 0, etc., but the output look like this: 
```{r pollination_combined_mle, warning=FALSE}
mle2(poll.NLL, start=list(lambda=1, phi=0.5), data=list(visits=visits, fert=fert, size=20) )
```

We have just used the two distinct datasets to estimate these two parameters! Very cool! (The actual values are nearly same as what we got using the naive, two-step approach above, but that will depend a lot on on both the data and the model formulation. In the long run, this approach will get you to the right answer a lot more reliably. It also gets you a likelihood and the ability to calculate confidence intervals, but that is next week!)


Homework
--------

The homework for this lab is to fit two models to a data set using likelihood. We'll use a simple data set and two simple, but realistic models.

**The problem**: When large aquatic mammals die in the oceans, they fall to the sea floor and support a large assemblage of scavengers and decomposers. Most decomposition occurs during a "mobile-scavenger stage lasting months to years, during which aggregations of sleeper sharks, hagfish, rat-tails and invertebrate scavengers remove whale soft tissue at high rates" (Smith and Baco 2003). Such "Whale falls" must attract these scavengers from long distances. Smith and Baco (2003) present estimates of the mass and rates of decomposition of five natural and experimentally implanted whale carcasses off the coast of California, as well as the rates of decomposition for three small cetaceans observed in the North Atlantic (Jones et al. 1998). 

There are two questions I would like you to address with these data:

1. How does scavenging rate change with the mass of the carcass?
2. Does scavenging of whales in the North Atlantic differ from Southern California?

I would like you to use likelihood methods to answer these questions to the best of your abilities (and the best of the data).


**The data**: These data are "scavenged" from the Smith and Baco (2003) paper. They can be found on the lab piazza site
```{r load.scavdata}
library(tidyverse)
scav <- read_csv("WhaleFallScavenging.csv")
scav
```
`Location` is either Southern Californica (`SoCal`) or North Atlantic (`NAtl`).   
`CarcWt` is the mass of the carcass in $kg$.   
`ScavRt` is the scavenging rate in $kg/day4.   
```{r plot.scavdata}
ggplot(data=scav, aes(x = CarcWt, y = ScavRt, color = Location)) + geom_point()
```

Maybe try changing the scale of the x-axis with `scale_x_log10()`.

**The deterministic models**: If you plot the scavenging rate against the log of carcass mass, you might guess that the rate of scavenging increases linearly as the mass doubles (i.e., an exponential model). Indeed, the Smith and Baco (2003) used this model. Interesting this implies that carcasses that are ~ 5000 $Kg$ have already attracted 2/3rds of the scavengers in the area; it takes more and more mass to attract those few, reticent scavangers. 

Alternatively, we might expect that the scavenging rate increases linearly with surface area of the whale. Cribbing from Woodward et al. (2006), we can expect the surface area ($SA$) in $m^2$ to increase with $\text{Mass}$ in $Kg$ as:
$$
  \begin{aligned}
		SA \approx 0.08 \times \text{Mass}^{0.65}
  \end{aligned}
$$

If you can think of alternative models (monomolecular or Michaelis-Menton anyone?), try that as well.

**The stochastic distribution**: Given what you know about the data, either from first principles or how it looks, choose a stochastic distribution to represent the error around the deterministic expectations. With so few observations it might be hard to be certain, but that's OK. Just make sure you understand what parameter(s) it requires and what they do.

**The likelihood functions**: You will need to write functions to calculate the negative log-likelihood for each model you are considering. One hint: depending on what function you decide to use to fit your model to the data (see below) you may want to write your function slightly differently. 

**Model fitting**: You are welcome to use `optim()` or the `mle2()` function in `bbmle` package. Either way, you need to fit your models (via your likelihood functions) to the data.

**What to submit**: I just want 1) a plot of your model predictions plotted on the scatter plot we've already made and 2) your MLEs and likeihood for each model.
