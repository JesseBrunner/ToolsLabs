---
title: 'Fitting models to data part 2: likelihood, by hand and automagically'
author: "Jesse Brunner"
date: "February 17, 2015"
output: html_document
---


This week we will learn to fitting models to data using likelihood as our criterion for fit. Specifically our goals are to:

* calculated the likelihood of a simple data set by hand
* learn to use R functions to calculate likelihood for you
* find the parameter values that maximize the likelihood
* automating the maximizing of likelihood with optim()


A simple example, from class
----------------------------

In class I gave you two problems to work through by hand, with a calculator. Let's revisit these examples using R for the heavy lifting. 

The first problem was this: imagine we collected 9 wood frog tadpoles from a pond, brought them back to lab, and found that 4 were infected. What is our best estimate of the probability of infection in the pond, which we will call $\pi$? In likelihood we simply flip this on its head, asking what value of $\pi$ maximizes the probability of seeing our data? The idea is that the parameter value that is most likely to give us our data is the most likeliy, given our data. 

For our data, 4 out of 9 infected (4 successes in 9 Bernouli trials) the likelihood is provided by the binomial: 

$$ \mathcal{L}(\pi | 4 \mbox{~of~} 9) = P(4 \mbox{~of~} 9 | \pi) = \binom {9}{4} \pi^4 (1-\pi)^5. $$

Notice that $\pi$ is the only parameter in the equation; the data are fixed. So we can choose a value of $\pi$ and then calculate its likelihood, and repeat until we find a value that gives us the maximum $\mathcal{L}$. For example, the likelihood of $\pi = 0.1$ is

$$ \mathcal{L}(\pi = 0.1 | 4 \mbox{~of~} 9) = P(4 \mbox{~of~} 9 | \pi = 0.1) = \binom {9}{4} 0.1^4 (1-0.1)^5. $$

In R we can calculate this as (remembering that $\binom {9}{4} = \frac{9!}{4!(9-4)!}$)
```{r likelihood_1}
factorial(9)/(factorial(4) * factorial(5)) * 0.1^4 * 0.9^5
# which is the same as
choose(n=9, k=4) * 0.1^4 * 0.9^5
```

If you remember back to last week, though, you will recall that R has built-in functions to provide the probability (density or mass), like _dbinom(). (If you don't remember, look back at last week's lab.) Before we had the probability of success fixed and the found the probability of different outcomes. Now we are interested in holding the outcomes (our data!) constant and varying the probability of success. So for this data set, the probability is
```{r likelihood_2}
dbinom(x=4, size=9, prob=0.1)
```
This makes it a lot easier to try lots of values as well as plot the likelihood as a function of different values of $\pi$. 
```{r likelihood_plot, fig.width=5, fig.height=4}
pis <- 1:99/100
Ls <- dbinom(x=4, size=9, prob=pis)
library(ggplot2)
qplot(pis, Ls) + geom_vline(xintercept=4/9)
```

A slightly more complicated example, from class
-----------------------------------------------

**Model 1: A frog's a frog**
To complicate things a bit, I added a second sample (of green frog tadpoles) in which 2 of 5 were infected. We are assumint that the probability of infection, $\pi$, is the same between species. The likelihood of both samples (the first AND the second) is just the product of the binomial probability of each sample:
$$ 
  \begin{aligned}
  \mathcal{L} (\pi | 4 \mbox{~of~} 9 \cap 2 \mbox{~of~} 5) 
  &= P(4 \mbox{~of~} 9 | \pi ) \times P(2 \mbox{~of~} 5 | \pi) 
  &= \binom {9}{4} \pi^4 (1- \pi)^5 \times \binom {5}{2} \pi^2 (1 - \pi)^3
  \end{aligned}
$$

In general the likelihood of multiple samples, each of size $n_i$ with probability of success, $\pi$, is just the product of the likelihood of each sample: 
$$
	 \mathcal{L} = \prod\limits_{i=1}^N \binom{n_i}{k_i} \pi^{k_i}(1-p)^{n_i-k_i}.
$$

So to make sure you can do this, calculate the likelihood of this full data set with the two samples. (Note, the answer should be `r round(dbinom(x=4, size=9, prob=6/14)*dbinom(x=2, size=5, prob=6/14), 4)`. If you are not gettin this, remember that in this simple case, the $\pi_{MLE} = 6/14$.)`

As we observed in class (and you can quickly verify for yourself using R), the likelihood values get smaller as the sample size increases. This causes computational problems when these products of small numbers get closer and closer to zero. So instead we usually work in terms of the log-likelihood (LL), which is:
$$
	 LL = \sum\limits_{i=1}^N \left( \log  \binom{n_i}{k_i} +k_i\log \pi + (n_i-k_i) \log(1-\pi) \right)
$$
Note that working with the log-likelihood we now work with the sums. This makes it much easier to calculate L for larger sample sizes. In R we can simply calculate the LL by specifying `log=TRUE` in the `dbinom()` (or equivalent) function. 

So our log-likelihood for the above example is:
```{r logLik}
dbinom(x=4, size=9, prob=6/14, log=TRUE) + dbinom(x=2, size=5, prob=6/14, log=TRUE)
``` 
And the plot we created above is 
```{r loglikelihood_plot, fig.width=5, fig.height=4}
LLs <- dbinom(x=4, size=9, prob=pis, log=TRUE) + dbinom(x=2, size=5, prob=pis, log=TRUE)
qplot(pis, LLs) + geom_vline(xintercept=6/14)
```

In actual fact, though, we tend to work on the _negative_ log-likelihood (NLL) because minimization problems are a bit easier to code than maximization problems (just like we did with sums-of-squares two weeks ago) and because this quantity is used in metrics of fit like AIC. It is honestly just a matter of putting a minus sign in front of the equation or code.

```{r NLL_plot, fig.width=5, fig.height=4}
-( dbinom(x=4, size=9, prob=6/14, log=TRUE) + dbinom(x=2, size=5, prob=6/14, log=TRUE) )
NLLs <- -( dbinom(x=4, size=9, prob=pis, log=TRUE) + dbinom(x=2, size=5, prob=pis, log=TRUE) )
qplot(pis, NLLs) + geom_vline(xintercept=6/14)
```

Again, we put a vertical line at $\pi_{MLE}$. It may be difficult for you to see that this is really the minimum (i.e., the MLE), but computurs are pretty good at figuring out these small differences. 

Finding the maximum likelihood with optimization
-----------------------------------------------

Ideally (and soon, necessarily) we want to automate the way we find MLEs. In outline, it is very similar to what we did with the sums-of-squares before. In fact, we will start with the 'optim()' function like we used two weeks ago before learning a slightly snazzier function. 

First we need a function that takes our data (both $n$ & $k$ for each sample) and our parameter(s), here $\pi$ and then returns the NLL.
```{r NLL_fxn}
binom.NLL <- function(n, k, pi){
	-sum( dbinom(x=k, size=n, prob=pi, log=TRUE) )
}
```
(You may be wondering why we're bothering writing our function so it can handle long vectors of numbers, using the `sum()` function and all rather than a simple `+`. We only have two sample, after all! Well, most of the time we will have much larger data sets where keeping track of all of the `+`'s would be difficult, but more importantly we should try to write our functions so that they can handle _any_ dataset that is similar to what we have in hand. This formulation, which works on vectors, can handle vectors of length 2, which we have here, to much longer ones.)

Let's make sure that this works with our data from the example above.
```{r NLL_fxn_test}
tested <- c(9,5)
infected <- c(4,2)
binom.NLL(n=tested, k=infected, pi=6/14)
```
OK, the function works. Now we can try to use 'optim()` to automatically find $\pi_{MLE}$, which should be 6/14 = `r 6/14`. 
```{r NLL_optim}
optim(fn=binom.NLL, par=c(pi=0.5), n=tested, k=infected, 
			method="Brent", lower=0.1, upper=1) 
```
Success! So we can automagically find the MLE for our simple model and, just like before, with models that have multiple parameters. In the longer run, `optim()` has some rough edges and makes a few things we'd like to do a bit difficult. Thankfully, Bolker has written a package called `bbmle` that has function, `mle2()`, that can fit likelihood models for us (it calls `optim()` for us and takes care of a lot of niggly bits). In outline it works the same, but it prefers both a list (rather than a vector) of initial starting values and a list of our data. 
```{r NLL_mle2}
library(bbmle)
mle2(binom.NLL, start=list(pi=0.5), data=list(n=tested, k=infected))
```
Why do we get slightly different values with `mle2()` than from `optim()`? It comes down to different default optimization algorithms. We could, however, get the same exact answer if we said `method="Brent, lower=0.1, upper=1` in the call to `mle2()`. It is, afterall, just a nice wrapper around the call to `optim()`. So they may seem very similar, but in the longer run `mle2()` has some very nice bells and whistles, so we will stick with it.

**Model 2: A wood frog is not a green frog**
So the alternative model is that the probability of infection is different for each species, that is $\pi_{wf} \neq \pi_{gf}$. Overall, our likelihood looks like it did above, only with two different $\pi$s.
$$ 
  \begin{aligned}
  \mathcal{L} (\pi_{wf}, \pi_{gf} | 4 \mbox{~of~} 9 \cap 2 \mbox{~of~} 5) 
  &= P(4 \mbox{~of~} 9 | \pi_{wf} ) \times P(2 \mbox{~of~} 5 | \pi_{gf}) 
  &= \binom {9}{4} \pi_{wf}^4 (1- \pi_{wf})^5 \times \binom {5}{2} \pi_{gf}^2 (1 - \pi_{gf})^3
  \end{aligned}
$$

How do we code this in R? Well, we just need to change our `binom.NLL()` function to accomodate the two sets of data with different parameters. We need it to take as an argument a vector telling it which group is which (here, `spp`) and then also the two parameters. Within the function, then, it just needs to use the right parameter for each species/group. 

```{r NLL_fxn2}
binom.NLL2 <- function(n, k, spp, piWF, piGF){
	-sum( dbinom(x=k, size=n, 
				prob= ifelse(spp == "WF", piWF, piGF), 
				log=TRUE) )
}
```
Notice that now prob is either `piWF` (when `spp == "WF"` is `TRUE`) or `piGF` (when `spp == "WF"` is `FALSE`). Let's see what we get.

```{r NLL_fxn2_mle2}
# create a vector species
spp <- c("WF", "GF")
# use mle2 to fit the model with _two_ parameters to the data
mle2(binom.NLL2, start=list(piWF=0.5, piGF=0.5), data=list(n=tested, k=infected, spp=spp))
```
We should see that the estimate of $\pi_{wf}$ is the same as the MLE for $\pi$ in the first dataset with only wood frogs. The estimate for $\pi_{gf}$ should confirm your intuition, too.

--------------------

_Optional!_
This first way to do things is robust and will always work. If your head is swimming, then maybe skip to the next section. You don't need the second way of doing things, but it can be nice. If you're still holding on, let's see the second way to fit a model with different parameters for each species. It depends on the internal magic of the `mle2()` function and will thus take a bit of explanation. 

In the `mle2()` function we can specify our likelihood as a _formula_ rather than as a separate function. Let's see the example for model 1, with a single common value of $\pi$. 
```{r mle2_wizardry_1}
mle2(infected ~ dbinom(size=tested, prob=pi),
		 start=list(pi=0.5), 
		 data=list(tested, infected)
		 )
```
The formula is `infected ~ dbinom(size=tested, prob=pi)`, which reads, "the number infected is binomially distributed with the `size` of each trial equal to the number tested and a probability (`prob`) of "success" that is a constant called `pi`." (If you look at the help file for `dbinom()` you will see that the first argument it takes is `x`, the number of "successes". The `mle2()` function knows that whatever is on the left side of the equation (left of the tilde) is this `x`. Everything else needs to be inside `dbinom()` part of the equation. Also note that `mle2()` knows to set `log=TRUE` when it calls `dbinom()`. ) So this formula notation simplifies things a bit and, provided you are careful, can make your code a lot cleaner. It won't work in every case, but it is a good bit more flexible than I've shown (and as we'll see next week). 

Now on to how to have different parameters for each group. _If_ we use the formula notation, it becomes fairly simple. We can make the parameter `pi` a linear function of some other variable, like `spp`. 
```{r mle2_wizardry_2, eval=FALSE}
mle2(infected ~ dbinom(size=tested, prob=pi),
		 start=list(pi=0.5), 
		 data=list(tested, infected, spp), #add "spp" to the data
		 parameters=list(pi ~ spp) # add a "parameters" option saying that "pi" is a function of "spp"
		 )
```
Note the two things I've added. The most important part is is the `parameters` option. This uses the same formula notation as linear regressions, etc. We could, for instance, say `parameters=list(pi ~ spp + age + pond)` if we provided data on the age of the individual and the pond, etc. 

The results of this call look like this:
```{r hidden, echo=FALSE, warning=FALSE}
a <- mle2(infected ~ dbinom(size=tested, prob=pi),
		 start=list(pi=0.5), 
		 data=list(tested, infected, spp), #add "spp" to the data
		 parameters=list(pi ~ spp) # add a "parameters" option saying that "pi" is a function of "spp"
		 )
a
```

You may be a bit confused by the output. You get a `pi.(Intercept)` and a parameter called `pi.sppWF`. This follows the linear regression notation where there is an intercept for the "base" group (here, the first level of `spp`, which is defined, by default alphabetically, as `GF`) and then an added effect of being in the other group (`spp` = `WF`). You can certainly recover the same values we observed before (with a bit of rounding error). $\pi_{gf}$ = `pi.(Intercept)` = `r coef(a)[1]` and $\pi_{wf}$ = `pi.(Intercept)` + `pi.sppWF` = `r coef(a)[1]` + `r coef(a)[2]`= `r sum(coef(a))`. However, if we wanted we could instead fit this model without an intercept, which would yeild estimates of $\pi$ for each species separately. To do so we use the `-1` to say "no intercept".
```{r mle2_wizardry_3, warning=FALSE}
mle2(infected ~ dbinom(size=tested, prob=pi),
		 start=list(pi=0.5), 
		 data=list(tested, infected, spp), 
		 parameters=list(pi ~ -1 + spp) 
		 )
```

--------------------

An example with two parts to the likelihood!
-------------------------------------

Let us look at another example that illustrates the flexibility of this approach. Imagine that you were interested in pollination of a certain rare flower. Specifically you wanted to estimate pollinator visitation rates ($\lambda$) as well as the probability that a flower is fertilized given a visit ($\phi$) so that you can get a sense of whether these flowers are pollinator-limited. Estimating pollinator visitation rates is straightforward, if not easy. Just sit in a lawn chair with a beer in hand recording how many visits each of several focal flowers get per day (or hour or whatever). Take a guess: how would these visitation data be distributed? 

The second part, though, is a bit tougher in nature (although I'm sure you could imagine running around with a Q-tip and pollen in the lab or other similar experiments). Generally what you could do is measure the number or fraction of lowers that produced seeds. How do you think these data would be distributed? 

The key to the problem is to realize that the probability of being fertilized (and thus having a seed, ignoring the possible events after fertilization) is a function of both the total number of visits to a flower (which is a function of visitation rate times the time, $t$, the flower is open) and the probability of being pollinated given a visit. We might start by writing down:

$$
	P(pollinated) = \lambda \times t \times \phi,
$$

but in fact this would not quite be right. Once a flower is pollinated, it can't be pollinated again, so none of the extra visits that might have been successful count. So instead what we want to do is calculate the probability that a flower is _not_ pollinated and subtract that from one, which is

$$
	P(pollinated) = 1-(1-\phi)^{\lambda \times t}.
$$

Make sure you follow the logic before we move on.

Right, so back to our data. We have 10 observations of visitation rates (numbers of visits to a flower per day):
```{r data1}
visits <- c(4, 3, 1, 1, 4, 3, 0, 1, 0, 2)
```
and we also came back and collected 15 samples of 20 flowers each to measure the proportion that were fertilized:
```{r data2}
fert <- c(16, 13, 15, 15, 11, 13, 13, 17, 17, 19, 10, 13, 15, 12, 14)
```

So let's be clear about the likelihood of each part. Assuming that flowers are visited randomly by pollinators, we would expect our data to be Poisson distributed with a rate paramter $\lambda$ (I sort of telegraphed that, didn't I?) and the fertilization data would be binomially distributed with a probability of success = $P(pollinated)= 1-(1-\phi)^{\lambda \times t}$. For simplicity let's assume that all flowers are open for exactly 5 days. 

Let's build up our likelihood analysis in a few steps, starting with the visitation data. We need a function that gives us the NLL of our data (`visits`) given a value of $\lambda$. 
```{r pollination_visits_NLL}
visits.NLL <- function(visits, lambda){
	-sum( dpois(x=visits, lambda=lambda, log=TRUE) )
}
```
This should look fairly similar to what we did in the previous examples. If you are getting lost, compare this to `binom.NLL()` above. 

So let's find the $\lambda_{MLE}$.
```{r pollination_visits_MLE}
mle2(visits.NLL, start=list(lambda=1), data=list(visits=visits))
```
```{r echo=FALSE,warning=FALSE}
lam <- mle2(visits.NLL, start=list(lambda=1), data=list(visits=visits))
```

We can use the same approach to estimate the probability a flower is pollinated from the `fert` data.
```{r pollination_fert_NLL, warning=FALSE}
# our NLL function assuming binomially distributed data
fert.NLL <- function(fert, size, probFert){
	-sum( dbinom(x=fert, size=size, prob=probFert, log=TRUE) )
}
# find the MLE of probFert
mle2(fert.NLL, start=list(probFert=0.5), data=list(fert=fert, size=20))
```
```{r echo=FALSE,warning=FALSE}
p <- mle2(fert.NLL, start=list(probFert=0.5), data=list(fert=fert, size=20))
```


So our best estimate of the visitation rate is `r round(coef(lam),4)` per day and our best estimate of the probability of fertilization over the 5d window is `r round(coef(p),4)`. Now how do we use these estimates to figure out the pollination probability, $\phi$? Well the naive approach, which is commonly used and reasonably acceptable, would be to plug these numbers into the equation for the probability of fertilization, which we set out above, and solve for $phi$:
$$
P(pollinated) = 1-(1-\phi)^{\lambda \times t} = 
`r round(coef(p),4)` = 1-(1-\phi)^{`r round(coef(lam),4)` \times 5}.
$$
By this method, $\phi =$ `r round( 1-(1-coef(p))^(1/(coef(lam)*5)), 4)`, which isn't a bad guess. It ignores the fact, though, that there is information on $\lambda$ in the `fert` data. That is, the `fert` data can, and should, be used to constrain our best guess at what $\lambda$ should be and thus what $\phi$ is. So how do we combined these data sets to estimate both parameters simultaneously? It is actually not too difficult. We just need to calculate the NLL for each part and then add them together. (Remember, we are interested in the probability of seeing the `visits` data AND the `fert` data, so we need to multiply their likelihoods, but because we are working on the _log_-likelihood scale, we just add them.)
```{r pollination_combined_NLL}
poll.NLL <- function(visits, lambda, fert, size, phi){
	# neg log-likelihood of Poisson-distributed visits
	NLL.visits <- -sum( dpois(x=visits, lambda=lambda, log=TRUE) )
	
	# calculate probFert
	probFert <- 1-(1-phi)^(lambda*5)
	# neg log-likelihood of binomially-distributed fertalization data
	# given a probability of fertilization as calculated
	NLL.fert <- -sum( dbinom(x=fert, size=size, prob=probFert, log=TRUE) )
	
	return(NLL.visits + NLL.fert)
}
```
(Notice that we calculate the `probFert` value using both `phi` and `lambda`, but those values are whatever is given to the function by `mle2()`, they do not change. We are _not_ first finding the MLE of `lambda` and then using that to calculate `probFert`; we could calculate `probFert` right at the start and it would work the same.)

We'll get some errors because the optimization routine is going below 0, etc., but the output look like this: 
```{r pollination_combined_mle, warning=FALSE}
mle2(poll.NLL, start=list(lambda=1, phi=0.5), data=list(visits=visits, fert=fert, size=20) )
```

We have just used the two distinct datasets to estimate these two parameters! Very cool! (The actual values are nearly same as what we got using the naive, two-step approach above, but that will depend a lot on on both the data and the model formulation. In the long run, this approach will get you to the right answer a lot more reliably. It also gets you a likelihood and the ability to calculate confidence intervals, but that is next week!)


Homework
--------

*  Catch up on things
*  Relax
*  Think
*  Think some more

